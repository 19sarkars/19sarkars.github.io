{
  "hash": "5ade7e62b845bf56e7b022e6465dc6ce",
  "result": {
    "markdown": "---\ntitle: 'Linear and Nonlinear Regression'\nauthor: 'Shivangi Sarkar'\ndate: '2023-11-16'\ncategories: ['Regression', 'Prediction']\ndescription: 'An introduction to linear and nonlinear regression.'\nexecute: \n  message: false\n  warning: false\neditor_options: \n  chunk_output_type: console\n---\n\n# What is linear regression?\n\nLinear regression is simply a model that estimates the relationship one independent variable and one dependent variable. This relationship is assumed to be linear, and is therefore represented by a straight line.\n\n# Applications of linear regression\n\nLinear regression is applied in various areas:\n\n-   Medicine: It can be used to find correlations between various factors such as age, body weight, and blood pressure\n-   Environmental health: The relationships between natural elements can be evaluated\n-   Sports analysis: Team and player performance in a game can be analyzed\n\n# What is nonlinear regression?\n\nNonlinear regression differs from linear regression in that it assumes that the dependent variable is a nonlinear function of the independent variables. This means the relationship can appear exponential, polynomial, or logarithmic.\n\n# Applications of nonlinear regression\n\nNonlinear regression can also be applied in various ways:\n\n-   Agricultural research: Crops and soil processes are better captured as nonlinear models\n-   Economic growth: Logistic models can be used to predict population growth over time\n-   Drug response modeling: Nonlinear regression can come in useful when modeling the relationship between drug dosage and physiological responses.\n\n# How do I choose between linear and nonlinear regression?\n\nIn order to decide between linear and nonlinear regression for a data set, you must consider the following factors:\n\n-   The purpose of your data\n-   The quality of your data\n-   The number of independent variables\n-   The criteria you use to evaluate your models\n\nFirst, you should plot your data and visually inspect the relationship between variables. If the data points align well as a line, linear regression should be chosen. If curved, nonlinear regression is the better option. Sometimes this may not be enough and you may have to go through a process of trial and error to determine the optimal regression method.\n\n## Code example\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn.datasets import load_breast_cancer\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# First we will load the dataset we will be using.\ncancer = load_breast_cancer()\n\n# load the dataframe\ndf = pd.DataFrame(cancer.data, columns = cancer.feature_names)\n\n# add labels\ndf['target'] = cancer.target\n\n# Let's get a preview of what the data looks like\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean radius</th>\n      <th>mean texture</th>\n      <th>mean perimeter</th>\n      <th>mean area</th>\n      <th>mean smoothness</th>\n      <th>mean compactness</th>\n      <th>mean concavity</th>\n      <th>mean concave points</th>\n      <th>mean symmetry</th>\n      <th>mean fractal dimension</th>\n      <th>...</th>\n      <th>worst texture</th>\n      <th>worst perimeter</th>\n      <th>worst area</th>\n      <th>worst smoothness</th>\n      <th>worst compactness</th>\n      <th>worst concavity</th>\n      <th>worst concave points</th>\n      <th>worst symmetry</th>\n      <th>worst fractal dimension</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.3001</td>\n      <td>0.14710</td>\n      <td>0.2419</td>\n      <td>0.07871</td>\n      <td>...</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.1622</td>\n      <td>0.6656</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.0869</td>\n      <td>0.07017</td>\n      <td>0.1812</td>\n      <td>0.05667</td>\n      <td>...</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.1238</td>\n      <td>0.1866</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.1974</td>\n      <td>0.12790</td>\n      <td>0.2069</td>\n      <td>0.05999</td>\n      <td>...</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.1444</td>\n      <td>0.4245</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.2414</td>\n      <td>0.10520</td>\n      <td>0.2597</td>\n      <td>0.09744</td>\n      <td>...</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.2098</td>\n      <td>0.8663</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.1980</td>\n      <td>0.10430</td>\n      <td>0.1809</td>\n      <td>0.05883</td>\n      <td>...</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.1374</td>\n      <td>0.2050</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 31 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nX = df.drop('target', axis=1)\ny = df['target']\n\n# Now let's split the dataset into a training and testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Preprocess the data\n# Standardize the features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Create a logistic regression model\nmodel = LogisticRegression()\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Make predictions with the test data\ny_pred = model.predict(X_test)\n\n# Check accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.97\n```\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Let's get a visual representation of the model's performance\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nmatrix = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(6, 6))\nsns.heatmap(matrix, annot=True, fmt=\".0f\", linewidths=0.5, square=True, cmap='Blues')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](post_files/figure-html/cell-4-output-1.png){width=488 height=468}\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.metrics import roc_curve, auc\n\ny_probs = model.predict_proba(X_test)[:, 1]\nfpr, tpr, thresholds = roc_curve(y_test, y_probs)\nroc_auc = auc(fpr, tpr)\n\n# Plot ROC Curve\nplt.figure(figsize=(8, 8))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {roc_auc:.2f}')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc='lower right')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](post_files/figure-html/cell-5-output-1.png){width=663 height=671}\n:::\n:::\n\n\n",
    "supporting": [
      "post_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}